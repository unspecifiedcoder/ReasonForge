[
  {
    "task_id": "code-systems-001",
    "problem": "Design a URL shortening service (like bit.ly) that handles 100 million URLs per day. Describe the system architecture, database choice, hashing strategy, and how you would handle collisions.",
    "domain": "code",
    "difficulty": 5,
    "timeout_seconds": 360,
    "ground_truth": "Key components: (1) API servers behind a load balancer. (2) Use base62 encoding of an auto-incrementing counter or MD5/SHA256 hash truncated to 7 characters for short codes. (3) Database: NoSQL (DynamoDB/Cassandra) for high write throughput, keyed by short_code → {original_url, created_at, expiry}. (4) Cache layer (Redis/Memcached) for hot URLs (80/20 rule). (5) Collision handling: check-and-retry with hash, or use counter-based approach (no collisions). (6) Rate limiting per user/IP. (7) Analytics: async write to event queue. Scale: 100M/day ≈ 1157 writes/sec, easily handled by distributed DB.",
    "ground_truth_score": 1.0,
    "is_trap": false,
    "previously_unsolved": false,
    "tags": ["system-design", "url-shortener", "distributed-systems", "hashing"],
    "source": "benchmark",
    "author": "ReasonForge Team",
    "created_at": "2026-01-15"
  },
  {
    "task_id": "code-systems-002",
    "problem": "Design a distributed rate limiter that enforces API rate limits across multiple server instances. It must handle 50,000 requests per second and provide accurate limiting within a 1% error margin.",
    "domain": "code",
    "difficulty": 7,
    "timeout_seconds": 420,
    "ground_truth": "Approaches: (1) Token Bucket via Redis: use MULTI/EXEC with Lua script for atomicity. Key per user with token count and last refill timestamp. (2) Sliding Window Counter: combine fixed window counts with interpolation for the sliding effect—store counts in Redis with TTL. (3) For distributed consistency: use Redis Cluster with hash slots ensuring same user always hits same shard. (4) To handle Redis failures: local in-memory fallback with eventual sync. (5) For 50K RPS: Redis handles ~100K ops/sec per node, so 1-2 Redis nodes suffice. Use connection pooling. (6) Race conditions: Lua scripts in Redis are atomic. (7) Accuracy: sliding window log gives exact results but uses more memory; sliding window counter with interpolation gives <1% error.",
    "ground_truth_score": 1.0,
    "is_trap": false,
    "previously_unsolved": false,
    "tags": ["system-design", "rate-limiting", "distributed-systems", "redis"],
    "source": "benchmark",
    "author": "ReasonForge Team",
    "created_at": "2026-01-15"
  },
  {
    "task_id": "code-systems-003",
    "problem": "Design a real-time collaborative text editor (like Google Docs) that supports 100 concurrent users editing the same document simultaneously with sub-200ms latency.",
    "domain": "code",
    "difficulty": 9,
    "timeout_seconds": 600,
    "ground_truth": "Core algorithm: Use CRDTs (Conflict-free Replicated Data Types), specifically a sequence CRDT like RGA or LSEQ, OR use Operational Transformation (OT) with a centralized server. Architecture: (1) WebSocket connections for real-time bi-directional communication. (2) Each client maintains local document state and applies changes optimistically. (3) Operations sent to server which transforms/orders them and broadcasts to all clients. (4) For CRDT approach: each character has a unique ID (Lamport timestamp + site ID), insertions and deletions commute. (5) Cursor/selection sync via presence channel. (6) Persistence: periodic snapshots to database with operation log for recovery. (7) For 100 users: single server can handle this; for scale, partition by document. (8) Undo: maintain per-user operation stack with inverse operations.",
    "ground_truth_score": 1.0,
    "is_trap": false,
    "previously_unsolved": false,
    "tags": ["system-design", "CRDT", "real-time", "collaborative-editing"],
    "source": "benchmark",
    "author": "ReasonForge Team",
    "created_at": "2026-01-15"
  },
  {
    "task_id": "code-systems-004",
    "problem": "Design a distributed task queue (like Celery or AWS SQS) that guarantees at-least-once delivery, supports priority scheduling, dead-letter queues, and handles worker failures gracefully. Target throughput: 10,000 tasks/second.",
    "domain": "code",
    "difficulty": 7,
    "timeout_seconds": 420,
    "ground_truth": "Architecture: (1) Message broker: use a partitioned log (Kafka-like) or Redis Streams for ordering and persistence. (2) At-least-once delivery: workers ACK after completion; unACKed messages are re-enqueued after visibility timeout. (3) Priority: use multiple queues (high/medium/low) with weighted consumption, or a priority heap backed by sorted set in Redis. (4) Dead-letter queue: after N failed attempts, move to DLQ for manual inspection. Track retry count in message metadata. (5) Worker failure: heartbeat mechanism; coordinator reassigns tasks from dead workers. (6) Idempotency: consumers must handle duplicates (use task ID deduplication). (7) Persistence: WAL + periodic snapshots. (8) Scale: partition tasks by key, add consumers per partition. 10K/sec is achievable with Kafka or Redis Streams on modest hardware.",
    "ground_truth_score": 1.0,
    "is_trap": false,
    "previously_unsolved": false,
    "tags": ["system-design", "message-queue", "distributed-systems", "reliability"],
    "source": "benchmark",
    "author": "ReasonForge Team",
    "created_at": "2026-01-15"
  },
  {
    "task_id": "code-systems-005",
    "problem": "Design a content delivery network (CDN) from scratch. Explain how you would handle cache invalidation, content routing, origin shielding, and TLS termination for serving static assets globally with p99 latency under 50ms.",
    "domain": "code",
    "difficulty": 8,
    "timeout_seconds": 480,
    "ground_truth": "Architecture: (1) Edge servers (PoPs) in 50+ global locations, each with SSD-backed cache. (2) DNS-based routing: GeoDNS directs users to nearest PoP based on IP geolocation; anycast for additional resilience. (3) Cache strategy: pull-based (lazy loading on cache miss), with TTL and Cache-Control headers. (4) Cache invalidation: purge API that fans out to all PoPs via internal pub/sub; use surrogate keys for group invalidation; stale-while-revalidate for availability. (5) Origin shielding: intermediate cache layer between edge and origin to reduce origin load; only shield nodes fetch from origin. (6) TLS termination at edge with session resumption (TLS 1.3, 0-RTT). Certificate management via automated Let's Encrypt or managed PKI. (7) Consistent hashing for distributing content across cache nodes within a PoP. (8) Health checks and automatic failover between PoPs.",
    "ground_truth_score": 1.0,
    "is_trap": false,
    "previously_unsolved": false,
    "tags": ["system-design", "CDN", "caching", "networking", "global-scale"],
    "source": "benchmark",
    "author": "ReasonForge Team",
    "created_at": "2026-01-15"
  },
  {
    "task_id": "code-systems-006",
    "problem": "Design a search autocomplete system that serves suggestions within 100ms for a platform with 500 million queries per day. Describe data structures, ranking, and how you handle new/trending queries.",
    "domain": "code",
    "difficulty": 6,
    "timeout_seconds": 360,
    "ground_truth": "Architecture: (1) Data structure: Trie with top-K suggestions cached at each node, rebuilt periodically. For production, use a compressed trie (Patricia trie) or precomputed prefix → suggestions mapping. (2) Ranking: frequency-weighted, with time decay (recent queries weighted higher). Combine historical frequency with real-time trending signal. (3) Serving: prefix lookup from in-memory trie, return top 10 suggestions. (4) Updates: batch rebuild trie from query logs every 15 min; for real-time trending, maintain a streaming top-K (Count-Min Sketch + heap) that feeds into a fast-update layer. (5) Infrastructure: shard by prefix (a-m on shard 1, n-z on shard 2, etc.). Replicate each shard. (6) Personalization: blend global suggestions with user-specific recent queries stored in user session cache. (7) Filtering: remove offensive/inappropriate suggestions via blocklist.",
    "ground_truth_score": 1.0,
    "is_trap": false,
    "previously_unsolved": false,
    "tags": ["system-design", "autocomplete", "trie", "ranking", "real-time"],
    "source": "benchmark",
    "author": "ReasonForge Team",
    "created_at": "2026-01-15"
  },
  {
    "task_id": "code-systems-007",
    "problem": "Design a distributed consensus system for a 5-node cluster that maintains strong consistency for a key-value store. Explain leader election, log replication, and how the system handles network partitions.",
    "domain": "code",
    "difficulty": 9,
    "timeout_seconds": 540,
    "ground_truth": "Implement Raft consensus protocol. (1) Leader election: nodes start as followers with randomized election timeouts (150-300ms). On timeout, become candidate, increment term, vote for self, request votes from others. Win with majority (3/5). (2) Log replication: leader appends entries to log, sends AppendEntries RPCs to followers. Entry committed when replicated on majority. (3) Safety: election restriction ensures candidate's log is at least as up-to-date as majority. (4) Network partitions: minority partition cannot elect leader (need 3/5 votes), so no split-brain. Old leader in minority discovers new term and steps down. (5) Client interaction: linearizability via leader reads with read index or lease-based reads. (6) Membership changes: joint consensus or single-server changes. (7) Log compaction: snapshotting state machine periodically, truncating log.",
    "ground_truth_score": 1.0,
    "is_trap": false,
    "previously_unsolved": false,
    "tags": ["system-design", "consensus", "raft", "distributed-systems", "fault-tolerance"],
    "source": "benchmark",
    "author": "ReasonForge Team",
    "created_at": "2026-01-15"
  },
  {
    "task_id": "code-systems-008",
    "problem": "Design a real-time fraud detection system for a payment processor handling 5,000 transactions per second. The system must flag suspicious transactions within 50ms while maintaining a false positive rate below 0.1%.",
    "domain": "code",
    "difficulty": 8,
    "timeout_seconds": 480,
    "ground_truth": "Architecture: (1) Streaming pipeline: Kafka for ingestion → Flink/Spark Streaming for real-time processing. (2) Feature computation: maintain per-user sliding windows of transaction amounts, frequencies, locations in Redis (e.g., last 1h, 24h, 7d aggregates). (3) Rule engine: fast deterministic rules (velocity checks, geographic impossibility, amount thresholds) as first filter. (4) ML model: gradient-boosted tree or neural network model for scoring, served via low-latency model server (TensorFlow Serving). Features: transaction amount vs user average, time since last transaction, merchant category, device fingerprint, IP risk score. (5) Decision: combine rule score + ML score. Flag if above threshold. (6) Feedback loop: labeled outcomes feed back into model retraining (weekly batch). (7) For 50ms latency: pre-compute features, keep model in memory, parallelize feature fetch and model inference. (8) False positive minimization: calibrate threshold on validation set; use precision-recall curve to find 0.1% FPR operating point.",
    "ground_truth_score": 1.0,
    "is_trap": false,
    "previously_unsolved": false,
    "tags": ["system-design", "fraud-detection", "streaming", "ML-serving", "real-time"],
    "source": "benchmark",
    "author": "ReasonForge Team",
    "created_at": "2026-01-15"
  }
]
